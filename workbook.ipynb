{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.1 32-bit",
   "metadata": {
    "interpreter": {
     "hash": "e8293e6e10a54ddd8cddc9b5f007ef34192abaa3c6406753b0cc265dd57a9253"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ce314 Assignment 1\n",
    "# Dan Norstrom, dn18657@essex.ac.uk, 1807572\n",
    "\n",
    "\n",
    "# there are 3 showcases for Part 1A. one sturdiness test, one copypasted with bbc\n",
    "# and one webscraped with bbc, both the copy and the webscrape has the same result\n",
    "# so it's sufficiently sturdy. Note that they all use the same Regex. I've added the ability # to detect pence in addition to a single \"p\".\n",
    "\n",
    "# for part 1B we're simply checking that we can match all the number recieved in the exercise\n",
    "# The algorithm is sturdy with any type of seperator, here we're using a random word\n",
    "# some type of seperator is required as some number-patterns use whitespace inside the regex\n",
    "# this can also be resolved using newline. I'm using a motivational text as example.\n",
    "\n",
    "# For question 2 and 3 there is only markdown text, please ensure markdown is activated to\n",
    "# get the best reading experience, Thanks for taking your time reading and stay safe, Dan\n",
    "import nltk\n",
    "from nltk import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Filtered Exercise Text:\n['£50,000', '£117.3m', '30p', '500m euro', '338bn euros', '$15bn', '$92.88.', '100m Euro', '500 euros', '50 pence']\n\n"
     ]
    }
   ],
   "source": [
    "# Part 1A: Proof of sturdiness\n",
    "\n",
    "text = \"(20%) Write a regular expression that can find all amounts of money in a text. Your expression should be able to deal with different formats and currencies , for example £50,000 and £117.3m as well as 30p, 500m euro, 338bn eurosssssss, $15bn and $92.88. Make sure that you can at least detect amounts in Pounds, Dollars and Euros. 123m er  ert 100m Europians 500 euros 50 pence\"\n",
    "\n",
    "Showcase = re.findall(\n",
    "    \"([1-9]+[0-9\\.\\,]*[A-Za-z]* [Ee]uro[s]?)\"+\n",
    "    # start with 1+ number(s), number body, prefix, Ends with versions of Euros\n",
    "    # Examples: '338bn euros', '500m euro', '500 euros'\n",
    "\n",
    "    \"|([1-9]+[0-9\\.\\,]*[Pp])\"\n",
    "    # start with 1+ number(s), number body, ends with P or p for \"pence\"\n",
    "    # Examples: '30p'\n",
    "\n",
    "    \"|([1-9]+[0-9\\.\\,]* [Pp]ence?)\"\n",
    "    # start with 1+ number(s), number body, ends with versions of Pence\n",
    "    # Examples: '50 pence'\n",
    "\n",
    "    \"|([$£€]+[0-9\\.\\,]*[A-Za-z]*)\",text)\n",
    "    # Start with currency, number body, end with prefix\n",
    "    # Examples: '$15bn', '£117.3m'\n",
    "\n",
    "# cleaner\n",
    "y = []\n",
    "for first in Showcase:\n",
    "    for data in first:\n",
    "        y.append(data)\n",
    "\n",
    "yy = list(filter(None, y))\n",
    "print(f\"Filtered Exercise Text:\\n{yy}\\n\")\n",
    "\n",
    "# output: ['£50,000', '£117.3m', '30p', '500m euro', '338bn euros', '$15bn', '$92.88.', '100m Euro', '500 euros', '50 pence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Filtered BBC Text:\n['$131bn', '£100bn', '$100bn', '$17.4bn']\n\n"
     ]
    }
   ],
   "source": [
    "# Part 1A: For Higher marks: BBC output\n",
    "\n",
    "textBBC = \"The US economy expanded at an annual pace of 3% during the three months to the end of September, which was stronger than expected. The growth extended the robust activity reported in the previous quarter, when US GDP grew at an annual pace of 3.1%. Analysts had been expecting a sharp slowdown after back-to-back hurricanes battered several states in the quarter. But consumer spending held steady, despite a drop in homebuilding investment. Together the two quarters mark the strongest six months of economic activity for the US since 2014, the Commerce Department said. Overall, this is a very solid performance, given the disruption caused by Hurricanes Harvey and Irma, wrote Ian Shepherdson of Pantheon Macroeconomics. Their net effect seems to have been smaller and shorter than we expected. What went into the figure? Consumer spending, which increased at a hearty 3.3% rate in the second quarter, slowed to 2.4% growth - a deceleration probably caused by the hurricanes.Construction spending also fell, but exports and business investments in equipment and intellectual property accelerated from the previous quarter.Economists warned that estimates of business inventories, a major factor in the GDP rise, can vary significantly quarter-to-quarter.Excluding that category, GDP - a broad measure of goods and services made in the US - increased at an annual pace of 2.3%.The Commerce Department cautioned that its figures did not capture all the losses caused by the storms, which caused widespread closures of factories, offices and airports in states such as Florida and Texas.Its GDP estimates, for example, do not measure activity in US territories, such as Puerto Rico, which suffered some of the most severe damage.The Commerce Department estimated that storm-related damage to fixed assets, such as homes and government buildings, totalled more than $131bn (£100bn). It also said it expected the government and insurers to pay more than $100bn in insurance claims, with foreign companies accounting for more than $17.4bn.Commerce Department Secretary Wilbur Ross claimed Friday's GDP report a sign of progress, calling it a remarkable achievement in light of the recent hurricanes.President Donald Trump has made hitting annual GDP growth of 3% a goal, and pledged tax cuts and other policies intended to reach that pace or higher.President Trump's bold agenda is steadily overcoming the dismal economy inherited from the previous administration, Mr Ross said. As the President's tax cut plan is implemented, our entire economy will continue to come roaring back. On a year-on-year basis, GDP was up 2.3%, the Commerce Department said in its report, which is an advance estimate that will be revised as more data is collected. That pace is roughly in line with US expansion since the 2007-2009 recession. Kenneth Rogoff, a professor of economics at Harvard University, said the growth reflects improvement in the labour market and other areas that date back to the Obama administration. While some of the president's plans may boost growth, they're not in place yet, he said. Let's make no mistake - this was a very good number, he said. Jobs have been improving, consumption's been improving, businesses are doing better, there is a profound inequality problem but the US economy, despite not much from President Trump, has been doing well. Economists said the underlying economic strength shown in the report makes it more likely that central bankers at the US Federal Reserve will raise interest rates again by the end of the year, as expected. The price index for consumer spending, a closely-watched measure of inflation, increased at 1.3% in the third quarter, excluding food and energy. That remains below the Federal Reserve's 2% target.\"\n",
    "\n",
    "ShowcaseBBC = re.findall(\n",
    "    \"([1-9]+[0-9\\.\\,]*[A-Za-z]* [Ee]uro[s]?)\"\n",
    "    # start with 1+ number(s), number body, prefix, Ends with versions of Euros\n",
    "    # Examples: '338bn euros', '500m euro', '500 euros'\n",
    "\n",
    "    \"|([1-9]+[0-9\\.\\,]*[Pp])\"\n",
    "    # start with 1+ number(s), number body, ends with P or p for \"pence\"\n",
    "    # Examples: '30p'\n",
    "\n",
    "    \"|([1-9]+[0-9\\.\\,]* [Pp]ence?)\"\n",
    "    # start with 1+ number(s), number body, ends with versions of Pence\n",
    "    # Examples: '50 pence'\n",
    "\n",
    "    \"|([$£€]+[0-9\\.\\,]*[A-Za-z]*)\",textBBC)\n",
    "    # Start with currency, number body, end with prefix\n",
    "    # Examples: '$15bn', '£117.3m'\n",
    "\n",
    "# Cleaner\n",
    "y = []\n",
    "for first in ShowcaseBBC:\n",
    "    for data in first:\n",
    "        y.append(data)\n",
    "\n",
    "yy = list(filter(None, y))\n",
    "print(f\"Filtered BBC Text:\\n{yy}\\n\")\n",
    "\n",
    "# output: ['$131bn', '£100bn', '$100bn', '$17.4bn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Filtered Webscraped BBC Text:\n['$131bn', '£100bn', '$100bn', '$17.4bn']\n\n"
     ]
    }
   ],
   "source": [
    "# Part 1A: For full marks: BBC output WEBSCRAPED\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# here beautifulsoup is used to webscrape BBC, because of the way they handle extra posts\n",
    "# we need to decompose certain sections and tags as they've been worked into the article.\n",
    "# - i do hope that the tags such as \"css-94m6rd-HeadingWrapper\" keep the same id with your \n",
    "#   requests while testing this, oterwise you might get unintended text wrapped into the article\n",
    "r = requests.get(\"https://www.bbc.co.uk/news/business-41779341\") \n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "soup.find('header').decompose()\n",
    "soup.find('header', {\"class\", \"css-94m6rd-HeadingWrapper e1nh2i2l1\"}).decompose()\n",
    "soup.find('div', {\"class\", \"css-vk3nhx-ComponentWrapper e1xue1i80\"}).decompose()\n",
    "soup.find('section').decompose()\n",
    "soup.find('section').decompose()\n",
    "soup.find('aside').decompose()\n",
    "soup.find('aside').decompose()\n",
    "soup.find('aside').decompose()\n",
    "soup.find('footer').decompose()\n",
    "\n",
    "webscrapeBBC = soup.get_text()\n",
    "\n",
    "ShowcaseWebscrapeBBC = re.findall(\n",
    "    \"([1-9]+[0-9\\.\\,]*[A-Za-z]* [Ee]uro[s]?)\"\n",
    "    # start with 1+ number(s), number body, prefix, Ends with versions of Euros\n",
    "    # Examples: '338bn euros', '500m euro', '500 euros'\n",
    "\n",
    "    \"|([1-9]+[0-9\\.\\,]*[Pp])\"\n",
    "    # start with 1+ number(s), number body, ends with P or p for \"pence\"\n",
    "    # Examples: '30p'\n",
    "\n",
    "    \"|([1-9]+[0-9\\.\\,]* [Pp]ence?)\"\n",
    "    # start with 1+ number(s), number body, ends with versions of Pence\n",
    "    # Examples: '50 pence'\n",
    "\n",
    "    \"|([$£€]+[0-9\\.\\,]*[A-Za-z]*)\",webscrapeBBC)\n",
    "    # Start with currency, number body, end with prefix\n",
    "    # Examples: '$15bn', '£117.3m'\n",
    "\n",
    "# Cleaner\n",
    "y = []\n",
    "for first in ShowcaseWebscrapeBBC:\n",
    "    for data in first:\n",
    "        y.append(data)\n",
    "\n",
    "yy = list(filter(None, y))\n",
    "print(f\"Filtered Webscraped BBC Text:\\n{yy}\\n\")\n",
    "\n",
    "# output: ['$131bn', '£100bn', '$100bn', '$17.4bn']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Filtered Numbers: ['555.123.4565', '+1-(800)-545-2468', '2-(800)-545-2468', '3-800-545-2468 ', '555-123-3456 ', '555 222 3342', '(234) 234 2442', '(243)-234-2342', '1234567890', '123.456.7890', '123.4567 ', '123-4567 ', '1234567900', '12345678900']\n\n"
     ]
    }
   ],
   "source": [
    "# Part 1B: \n",
    "\n",
    "# seperating text with a random word so that numbers dont blend into eachother,\n",
    "# this should be alright as no further restrictions on seperator has been set in the assignment.\n",
    "textNr = \"555.123.4565 Coding \"+\\\n",
    "\"+1-(800)-545-2468 with \"+\\\n",
    "\"2-(800)-545-2468 nltk \"+\\\n",
    "\"3-800-545-2468 is \"+\\\n",
    "\"555-123-3456 a \"+\\\n",
    "\"555 222 3342 envigorating \"+\\\n",
    "\"(234) 234 2442 experience \"+\\\n",
    "\"(243)-234-2342 when \"+\\\n",
    "\"1234567890 combined \"+\\\n",
    "\"123.456.7890 with \"+\\\n",
    "\"123.4567 webscraping \"+\\\n",
    "\"123-4567 thats \"+\\\n",
    "\"1234567900 for \"+\\\n",
    "\"12345678900 sure \"\n",
    "\n",
    "ShowcaseNr = re.findall(\n",
    "    \"([0-9\\+\\-]*[\\(\\)0-9]+[\\.\\s\\-][0-9]*[\\.\\s\\-][0-9]*)\"\n",
    "    # 2 or 3 digit-groups with seperators, including possible region prefix\n",
    "    # Examples: \"123.456.7890\", \"(243)-234-2342\", \"+1-(800)-545-2468\"\n",
    "\n",
    "    \"|([0-9]+)\"\n",
    "    # Single body digits\n",
    "    # Examples: \"1234567900\"\n",
    "    ,textNr)\n",
    "\n",
    "# Cleaner\n",
    "y3 = []\n",
    "for first in ShowcaseNr:\n",
    "    for data in first:\n",
    "        y3.append(data)\n",
    "\n",
    "yy = list(filter(None, y3))\n",
    "print(f\"Filtered Numbers: {yy}\\n\")\n",
    "\n",
    "# output: ['555.123.4565', '+1-(800)-545-2468', '2-(800)-545-2468', '3-(800)-545-2468', '555-123-3456 ', '555 222 3342', '(234) 234 2442', '(243)-234-2342', '1234567890', '123.456.7890', '123.4567 ', '123-4567  ', '1234567900', '12345678900']"
   ]
  },
  {
   "source": [
    "#Part 2\n",
    "\n",
    "# we print both the top50 and their count as proof of how common they are.\n",
    "import string\n",
    "from nltk import re\n",
    "from nltk import word_tokenize\n",
    "from nltk import FreqDist\n",
    "from nltk.book import text7\n",
    "\n",
    "tokens = text7.tokens\n",
    "# substitute punctuation with empty,\n",
    "wordTokens = [re.sub(r'[^\\w\\s]','',word.lower()) for word in tokens if re.search(\"[\\w\\s]\", word)]\n",
    "countTokens = FreqDist(wordTokens)\n",
    "top_50 = countTokens.most_common(50)\n",
    "\n",
    "# Clean\n",
    "for x in top_50:\n",
    "    print(f\"{x[0]}: {x[1]}\")\n",
    "\n",
    "# output:\n",
    "# the: 4764\n",
    "# of: 2325\n",
    "# to: 2182\n",
    "# a: 2006\n",
    "# in: 1769\n",
    "# and: 1556\n",
    "# 1: 1166\n",
    "# 0: 1099\n",
    "# s: 869\n",
    "# for: 853\n",
    "# that: 848\n",
    "# t1: 806\n",
    "# u: 744\n",
    "# is: 672\n",
    "# said: 628\n",
    "# it: 577\n",
    "# on: 508\n",
    "# by: 440\n",
    "# at: 430\n",
    "# as: 415\n",
    "# 2: 402\n",
    "# with: 398\n",
    "# from: 391\n",
    "# million: 383\n",
    "# mr: 375\n",
    "# are: 369\n",
    "# was: 367\n",
    "# be: 356\n",
    "# t2: 345\n",
    "# its: 343\n",
    "# has: 339\n",
    "# an: 335\n",
    "# new: 328\n",
    "# have: 325\n",
    "# nt: 325\n",
    "# but: 309\n",
    "# he: 303\n",
    "# or: 294\n",
    "# will: 281\n",
    "# they: 263\n",
    "# company: 260\n",
    "# us: 253\n",
    "# which: 225\n",
    "# this: 224\n",
    "# says: 217\n",
    "# year: 214\n",
    "# about: 212\n",
    "# would: 209\n",
    "# more: 204\n",
    "# were: 197\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "the: 4764\nof: 2325\nto: 2182\na: 2006\nin: 1769\nand: 1556\n1: 1166\n0: 1099\ns: 869\nfor: 853\nthat: 848\nt1: 806\nu: 744\nis: 672\nsaid: 628\nit: 577\non: 508\nby: 440\nat: 430\nas: 415\n2: 402\nwith: 398\nfrom: 391\nmillion: 383\nmr: 375\nare: 369\nwas: 367\nbe: 356\nt2: 345\nits: 343\nhas: 339\nan: 335\nnew: 328\nhave: 325\nnt: 325\nbut: 309\nhe: 303\nor: 294\nwill: 281\nthey: 263\ncompany: 260\nus: 253\nwhich: 225\nthis: 224\nsays: 217\nyear: 214\nabout: 212\nwould: 209\nmore: 204\nwere: 197\n"
     ]
    }
   ]
  },
  {
   "source": [
    "# Part 3.1.1\n",
    "\n",
    "#p(x|y) = count(y->x)/count(y)\n",
    "\n",
    "a) <br>\n",
    "P(< /s>|Sam) = 2/5 <br>\n",
    "P(I|Sam) = 3/5 <br>\n",
    "answer: \"I\" has the highest probability<br>\n",
    "\n",
    "b) <br>\n",
    "P(like|do) = 1/2 <br>\n",
    "P(I|do) = 1/2 <br>\n",
    "answer: \"like\" & \"I\" have equal probability to occur <br>\n",
    "\n",
    "c) = a) <br>\n",
    "answer: \"I\". Because we use bigrams this answer will be equal to a). if we used trigrams the answer would different (< /s>) <br>\n",
    "\n",
    "d) <br>\n",
    "P(</s>|like) = 2/3 <br>\n",
    "p(Sam|like) = 1/3 <br>\n",
    "answer: \"< /s>\"\n",
    " "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Part 3.1.2\n",
    "a) <br>\n",
    "sentence: \" < s> Sam I do I like < /s> \" <br>\n",
    "P(Sam|< s>) = 3/5 <br>\n",
    "P(I|Sam) = 3/5 <br>\n",
    "P(do|I) = 1/5 <br>\n",
    "P(I|do) = 1/2 <br>\n",
    "P(like|I) = 2/5 <br>\n",
    "P(< /s>|like) = 2/3 <br>\n",
    "sentence probability = P(a) = 6/625 <br>\n",
    "\n",
    "b) <br>\n",
    "sentence: \"< s> Sam I am < /s>\" <br>\n",
    "P(Sam|< s>) = 3/5 <br>\n",
    "P(I|Sam) = 3/5 <br>\n",
    "P(am|I) = 2/5 <br>\n",
    "P(< /s>|am) = 1/2 <br>\n",
    "sentence probability: = P(b) = 9/125 <br>\n",
    "\n",
    "c) <br>\n",
    "sentence: \"< s> I do like Sam I am < /s>\"  <br>\n",
    "P(I|< s>) = 1/5 <br>\n",
    "P(do|I) = 1/5 <br>\n",
    "P(like|do) = 1/2 <br>\n",
    "P(Sam|like) = 1/3 <br>\n",
    "P(I|Sam) = 3/5 <br>\n",
    "P(am|I) = 2/5 <br>\n",
    "P(< /s>|am) = 1/2 <br>\n",
    "sentence probability = P(c) = 1/1250\n",
    "\n",
    "answer: b has the highest probability of 9/125 ( b > a > c)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Part 3.2\n",
    "\n",
    "perplexity is used to meassure how good a model is <br>\n",
    "e.g. good scores on correct sentences and bad scores while incorrect <br>\n",
    "<br>\n",
    "sentence: \"< s> I do like\" <br>\n",
    "<br>\n",
    "PP(sentence) = P(w1,w2,w3,w4)^(-1/N) <br>\n",
    "where N is the number of words in the sentence s to calculate perplexity PP <br>\n",
    "<br>\n",
    "further notes on begin/end-markers: <br>\"Since this sequence will cross many sentence <br>\n",
    "boundaries, we need to include the begin- and end-sentence markers < s> and < /s> <br>\n",
    "in the probability computation. We also need to include the end-of-sentence <br>\n",
    "marker (but not the beginning-of-sentence marker < s>) in the total count of word <br>\n",
    "tokens N\" ~Jurafsky and martin Chapter 3 p45 <br>\n",
    "<br>\n",
    "hence N is 3, not 4 as we do not include < s>. <br>\n",
    "<br>\n",
    "using the bigram probability chain rule we can say that: <br>\n",
    "P(w1,...,wn) = p(w1) x p(w2|w1) x p(w3|w1,w2) x p(w4|w1,w2,w3) <br>\n",
    "<br>\n",
    "we now have: <br>\n",
    "PP(sentence) = ( p(w2|w1) x p(w3|w2) x p(w4|w3) )^(-1/<N) <br>\n",
    "PP(sentence) = ( P(I|<s>) x  P(do|I) x P(like|do) )^(-1/N) <br>\n",
    "PP(sentence) = ( (1/5) x (1/5) x (1/2) )^(-1/3) <br>\n",
    "PP(sentence) = 3.68 <br>\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#Part 3.3.1\n",
    "\n",
    "if N is the total number of tokens in our training data, ignoring < s>.<br>\n",
    "N = 22<br>\n",
    "<br>\n",
    "if V is the unique words or \"vocabulary\" of our training data<br>\n",
    "V = 6<br>\n",
    "<br>\n",
    "Using Laplace smoothing we get:<br>\n",
    "P+1(wi|wi-1) = Count(wi-1, wi) +1 / Count(wi-1) + V <br>\n",
    "\n",
    "<br>\n",
    "P(do|< s>) = 1+1/5+6 = 2/11 <br>\n",
    "P(do|Sam) = 0+1/5+6 = 1/11 <br>\n",
    "P(Sam|< s>) = 3+1/5+6 = 4/11 <br>\n",
    "P(Sam|do) = 0+1/2+6 = 1/8 <br>\n",
    "P(I|Sam) = 3+1/5+6 = 4/11 <br>\n",
    "P(I|do) = 1+1/2+6 = 2/8  <br>\n",
    "P(like|I) = 2+1/5+6 = 3/11 <br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#Part 3.3.2\n",
    "\n",
    "using the Bigram LM trainingdata with Laplace we get: <br>\n",
    " <br>\n",
    "a)  <br>\n",
    "sentence = \"<s> do Sam I like\" <br>\n",
    "P(do|< s>) = 1+1/5+6 = 2/11 <br>\n",
    "P(Sam|do) = 0+1/2+6 = 1/8 <br>\n",
    "P(I|Sam) = 3+1/5+6 = 4/11 <br>\n",
    "P(like|I) = 2+1/5+6 = 3/11 <br>\n",
    "sentence probability = P(a) = 3/1331 <br>\n",
    " <br>\n",
    "b) <br>\n",
    "sentence = \"<s> Sam do I like\" <br>\n",
    "P(Sam|< s>) = 3+1/5+6 = 4/11 <br>\n",
    "P(do|Sam) = 0+1/5+6 = 1/11 <br>\n",
    "P(I|do) = 1+1/2+6 = 2/8 <br>\n",
    "P(like|I) = 2+1/5+6 = 3/11 <br>\n",
    "sentence probability = P(b) = 3/1331 <br>\n",
    " <br>\n",
    "answer: Both a) and b) have the same probability to occur: 3/1331 <br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}